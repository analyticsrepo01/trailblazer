{"cells":[{"cell_type":"markdown","id":"d144dc6e-a1ec-492d-991c-ed39e9bc2231","metadata":{"id":"d144dc6e-a1ec-492d-991c-ed39e9bc2231"},"source":["# Overview\n","\n","In this lab, you will be fine-tuning Google FLAN-T5 model with a custom dataset using Vertex AI Workbench with attached GPU(s)."]},{"cell_type":"markdown","id":"03659dbc-0a0b-45ad-bd3e-cd0028b2b2a8","metadata":{"id":"03659dbc-0a0b-45ad-bd3e-cd0028b2b2a8"},"source":["\n","# Goals\n","\n","The goals of this lab is for you to learn the end-to-end workflow to tune Google FLAN-T5 model through doing hands-on exercise."]},{"cell_type":"markdown","id":"bacdc579-b675-4445-bb0c-b3c69e6905b7","metadata":{"jp-MarkdownHeadingCollapsed":true,"tags":[],"id":"bacdc579-b675-4445-bb0c-b3c69e6905b7"},"source":["# Google FLAN-T5\n","\n","Flan-T5 is a large language model (LLM) developed by Google AI. It is a fine-tuned version of T5, which is a text-to-text transfer transformer. Flan-T5 is trained on a mixture of tasks, rather than a single task, which allows it to learn a more general-purpose representation of language. Flan-T5 includes the same improvements as T5 version 1.1, as well as the following new features:\n","\n","- Instruction finetuning: Flan-T5 is finetuned using a mixture of tasks, rather than a single task. This allows the model to learn a more general-purpose representation of language.\n","- Mixed-precision training: Flan-T5 is trained using mixed-precision training, which allows it to use more of the GPU's resources. This results in faster training times and better performance.\n","- Data augmentation: Flan-T5 uses data augmentation to artificially increase the size of the training dataset. This helps the model to learn more robust representations of language.\n","\n","<img src=\"https://camo.githubusercontent.com/623b4dea0b653f2ad3f36c71ebfe749a677ac0a1/68747470733a2f2f6d69726f2e6d656469756d2e636f6d2f6d61782f343030362f312a44304a31674e51663876727255704b657944387750412e706e67\" alt=\"Drawing\" style=\"width: 1080px;\"/>\n","\n","\n","It was released in the paper [Scaling Instruction-Finetuned Language Models](https://arxiv.org/pdf/2210.11416.pdf). Flan-T5 is a powerful language model that can be used for a variety of tasks, such as text generation, translation, summarization, and question answering.\n","\n","## Bias, risks, and limitations\n","The information below in this section are copied from the model's official paper:\n","\n","> Language models, including Flan-T5, can potentially be used for language generation in a harmful way, according to Rae et al. (2021). Flan-T5 should not be used directly in any application, without a prior assessment of safety and fairness concerns specific to the application.\n","\n","## Ethical considerations and risks\n","\n","Flan-T5 is fine-tuned on a large corpus of text data that was not filtered for explicit content or assessed for existing biases. As a result the model itself is potentially vulnerable to generating equivalently inappropriate content or replicating inherent biases in the underlying data.\n","\n","## Known limitations\n","\n","Flan-T5 has not been tested in real world applications.\n","\n","## Sensitive use:\n","\n","Flan-T5 should not be applied for any unacceptable use cases, e.g., generation of abusive speech."]},{"cell_type":"markdown","id":"45b097ff-6c36-413d-b8dc-b775c287c918","metadata":{"id":"45b097ff-6c36-413d-b8dc-b775c287c918"},"source":["# Environment setup"]},{"cell_type":"markdown","id":"8c907a46-f5de-4a35-acb0-c4bbfe59dccb","metadata":{"id":"8c907a46-f5de-4a35-acb0-c4bbfe59dccb"},"source":["## Check for attached GPU\n","\n","Fine-tuning models is a computationally intensive task. You will need a good GPU to support the workload. To check the attached GPU of this notebook instance, please run the following code:"]},{"cell_type":"code","execution_count":null,"id":"2113cbf1-15cb-4cfc-98c2-467dec65d087","metadata":{"id":"2113cbf1-15cb-4cfc-98c2-467dec65d087","outputId":"c4655373-a3c9-496c-e337-d2bc5d4a31f9"},"outputs":[{"name":"stdout","output_type":"stream","text":["GPU 0: NVIDIA A100-SXM4-40GB (UUID: GPU-b0c857a0-2e9a-5d49-8dd0-457178afa2fb)\n"]}],"source":["! nvidia-smi -L"]},{"cell_type":"markdown","id":"d89e8638-b3ea-4116-86ee-b98ccf7f4459","metadata":{"id":"d89e8638-b3ea-4116-86ee-b98ccf7f4459"},"source":["It should show something like \"GPU 0: NVIDIA A100-SXM4-40GB (UUID: GPU-XXX-XX)\""]},{"cell_type":"markdown","id":"aeb46f18-d5c6-4bc0-9dec-005f03364816","metadata":{"id":"aeb46f18-d5c6-4bc0-9dec-005f03364816"},"source":["## Install required packages\n","\n","To successfully run this notebook, you need to install the required packages. You can do this by executing following cell below."]},{"cell_type":"code","execution_count":null,"id":"5e3b9e67-c1b3-4d54-8b66-2ccfc3635b63","metadata":{"id":"5e3b9e67-c1b3-4d54-8b66-2ccfc3635b63"},"outputs":[],"source":["! pip install datasets py7zr transformers[torch] rouge-score nltk evaluate --upgrade"]},{"cell_type":"markdown","source":["⚠️⚠️⚠️⚠️⚠️ **In order to reflect the changes, you will need to restart the runtime after the installation.** ⚠️⚠️⚠️⚠️⚠️"],"metadata":{"id":"0Eyo5r0Joxyd"},"id":"0Eyo5r0Joxyd"},{"cell_type":"markdown","id":"2af8089a-7d49-4283-b3f9-e7f9e795dd8b","metadata":{"id":"2af8089a-7d49-4283-b3f9-e7f9e795dd8b"},"source":["# Dataset"]},{"cell_type":"markdown","id":"91700699-bd40-4538-ae31-6cea607f7272","metadata":{"id":"91700699-bd40-4538-ae31-6cea607f7272"},"source":["## Download dataset\n","\n","Here you will download the [samsum](https://huggingface.co/datasets/samsum) dataset. It is a collection of about 16,000 messenger-like conversations with summaries. The conversations were created by linguists fluent in English."]},{"cell_type":"code","execution_count":null,"id":"3ba9d6c8-8dae-427b-aaa0-7003e61c3ffc","metadata":{"colab":{"referenced_widgets":["4ed0dd46bfc34aeda809d383568f4e22","","aa7e9feaba0346e0bd37128c90c7fe94"]},"id":"3ba9d6c8-8dae-427b-aaa0-7003e61c3ffc","outputId":"e9935e6f-d544-42d7-fff7-6bc3f6c18868"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading and preparing dataset samsum/samsum to /home/jupyter/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4ed0dd46bfc34aeda809d383568f4e22","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/2.94M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Generating train split:   0%|          | 0/14732 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Generating test split:   0%|          | 0/819 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Generating validation split:   0%|          | 0/818 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Dataset samsum downloaded and prepared to /home/jupyter/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e. Subsequent calls will reuse this data.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"aa7e9feaba0346e0bd37128c90c7fe94","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from datasets import load_dataset\n","\n","# Define dataset ID to download\n","DATASET_ID = \"samsum\"\n","\n","# Load the defined dataset\n","dataset = load_dataset(DATASET_ID)"]},{"cell_type":"markdown","id":"237c35c1-2012-4903-8709-f6640bb76984","metadata":{"id":"237c35c1-2012-4903-8709-f6640bb76984"},"source":["Here you will analyze the data in a fair bit of detail. You will check the following:\n","\n","- The column names\n","- The column data types\n","- The amount of samples\n","- A few samples of the data"]},{"cell_type":"code","execution_count":null,"id":"de9c9bf5-34cc-4489-8f1f-08394a81d9cd","metadata":{"id":"de9c9bf5-34cc-4489-8f1f-08394a81d9cd","outputId":"9b70fb3b-d783-4629-b5aa-99c82a496ea2"},"outputs":[{"data":{"text/plain":["{'train': ['id', 'dialogue', 'summary'],\n"," 'test': ['id', 'dialogue', 'summary'],\n"," 'validation': ['id', 'dialogue', 'summary']}"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["# Find out column names\n","dataset.column_names"]},{"cell_type":"code","execution_count":null,"id":"f82c436d-3205-4ded-9ceb-b62164c913a6","metadata":{"id":"f82c436d-3205-4ded-9ceb-b62164c913a6","outputId":"af40267e-a221-42c8-ae24-c426c9090693"},"outputs":[{"data":{"text/plain":["{'id': Value(dtype='string', id=None),\n"," 'dialogue': Value(dtype='string', id=None),\n"," 'summary': Value(dtype='string', id=None)}"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["# Print out the value types of columns\n","dataset['train'].features"]},{"cell_type":"code","execution_count":null,"id":"79783fc0-6fa4-407a-b987-6a7af980ca55","metadata":{"id":"79783fc0-6fa4-407a-b987-6a7af980ca55","outputId":"dc50a909-8d5d-41cc-a98f-776826df6ac4"},"outputs":[{"name":"stdout","output_type":"stream","text":["train dataset: 14732 rows\n","test dataset: 819 rows\n","validation dataset: 818 rows\n"]}],"source":["# Check for number of samples\n","for key, value in dataset.shape.items():\n","    print(f\"{key} dataset: {value[0]} rows\")"]},{"cell_type":"code","execution_count":null,"id":"3fb0c38c-f6ae-4d61-a425-45d16555902a","metadata":{"id":"3fb0c38c-f6ae-4d61-a425-45d16555902a","outputId":"728123be-fd98-438a-9347-0571136a05b4"},"outputs":[{"name":"stdout","output_type":"stream","text":["DIALOGUE:\n","Isla: Why didn’t you tell me you were dating Mike?\n","Linda: I thought you wouldn’t like it\n","Isla: I don’t care what he does\n","Isla: Or who he dates\n","Isla: This chapter is over for me\n","Isla: All I can tell you is good luck\n","Isla: Maybe it will work out for you\n","\n","SUMMARY:\n","Linda didn't tell Isla about dating Mike. Isla is ok with it and hopes it will work out for them.\n","--------------------------------------------------------------------------------\n","DIALOGUE:\n","Lionel: What's your name?\n","Simona: You see my channel right?\n","Lionel: Yeah, what's your name?\n","Simona: Like, the one in the channel, dude. \n","\n","SUMMARY:\n","Simona's channels name is her real name too.\n","--------------------------------------------------------------------------------\n","DIALOGUE:\n","Caleb: Eva put channel 5 on\n","Eva: why???\n","Caleb: they're playing Broadchurch :D\n","Eva: whoa, thanks :D\n","\n","SUMMARY:\n","Channel 5 is airing Broadchurch.\n","--------------------------------------------------------------------------------\n"]}],"source":["import random\n","\n","# Print out a few samples from the train dataset\n","for i in random.sample(range(len(dataset[\"train\"])), 3):\n","    sample = dataset['train'][i]\n","    print(f\"DIALOGUE:\\n{sample['dialogue']}\\n\")\n","    print(f\"SUMMARY:\\n{sample['summary']}\")\n","    print(\"-\" * 80)"]},{"cell_type":"markdown","id":"982573d8-4f08-4a86-972d-3dd851b4955f","metadata":{"tags":[],"id":"982573d8-4f08-4a86-972d-3dd851b4955f"},"source":["## Prepare dataset\n","\n","FLAN-T5 requires token IDs as input. In this step, you will use the **AutoTokenizer** from the **transformers** library to tokenize the data and convert it to token IDs.\n","\n","To do this, you need to pass in the correct model ID to the **AutoTokenizer**. This will tell the tokenizer which tokenizer to initialize. It is important to use the correct tokenizer, as using the wrong tokenizer may not work or may create inaccurate tokens.\n","\n","If you know the required tokenizer for the FLAN-T5 model, you can also directly import it. For FLAN-T5, the tokenizer is **T5Tokenizer** or **T5TokenizerFast**."]},{"cell_type":"code","execution_count":null,"id":"8a19830c-2647-4909-ab77-e890f30b8f65","metadata":{"colab":{"referenced_widgets":["9a359a779c54404c838ee688a3486e43","6e11874392ea4e28bc7cbca394211e48","8c664109a2b34260914d0dde6bb7391a","eafae43c02c14f36aa552d5d93784a98"]},"id":"8a19830c-2647-4909-ab77-e890f30b8f65","outputId":"07375643-fbea-4c2b-bd6e-7394d862437d"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9a359a779c54404c838ee688a3486e43","version_major":2,"version_minor":0},"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6e11874392ea4e28bc7cbca394211e48","version_major":2,"version_minor":0},"text/plain":["Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8c664109a2b34260914d0dde6bb7391a","version_major":2,"version_minor":0},"text/plain":["Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"eafae43c02c14f36aa552d5d93784a98","version_major":2,"version_minor":0},"text/plain":["Downloading (…)cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from transformers import AutoTokenizer\n","\n","# Define the ID of the model\n","MODEL_ID = \"google/flan-t5-base\"\n","\n","# Load tokenizer of FLAN-t5-base\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)"]},{"cell_type":"code","execution_count":null,"id":"82b9268d-816b-4d0a-9cce-b680ac466e00","metadata":{"id":"82b9268d-816b-4d0a-9cce-b680ac466e00","outputId":"57ac95c7-827c-4cc8-8136-20dce19e6a72"},"outputs":[{"name":"stdout","output_type":"stream","text":["['▁Cal', 'e', 'b', ':', '▁Eva', '▁put', '▁channel', '▁5', '▁on', '▁Eva', ':', '▁why', '???', '▁Cal', 'e', 'b', ':', '▁they', \"'\", 're', '▁playing', '▁Broad', 'church', '▁', ':', 'D', '▁Eva', ':', '▁who', 'a', ',', '▁thanks', '▁', ':', 'D']\n"]}],"source":["# Print out tokenized words\n","print(tokenizer.tokenize(sample['dialogue']))"]},{"cell_type":"code","execution_count":null,"id":"1fe0255a-18f2-4b76-81e9-109660820362","metadata":{"id":"1fe0255a-18f2-4b76-81e9-109660820362","outputId":"a0f13c2d-9535-459c-a5a7-df142d9645c9"},"outputs":[{"data":{"text/plain":["{'input_ids': [3104, 15, 115, 10, 17627, 474, 4245, 305, 30, 17627, 10, 572, 8665, 3104, 15, 115, 10, 79, 31, 60, 1556, 13017, 28854, 3, 10, 308, 17627, 10, 113, 9, 6, 2049, 3, 10, 308, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["# The result after coverting to token IDs\n","tokenizer(sample['dialogue'])"]},{"cell_type":"markdown","id":"48d3dbae-9ec9-497f-84a9-169e365cfb50","metadata":{"id":"48d3dbae-9ec9-497f-84a9-169e365cfb50"},"source":["Abstractive summarization is a text-to-text generation task. That means the model will take a text as input and generate a summary (text) as output. Hence, before fine-tuning the model to perform summarization tasks, you need find out the maximum token legenth for both input and output texts.\n","\n","After knowing the maximum token length, you can truncate the sentences that are longer than the maximum token length or pad the sentences which are shorter. By doing this, all of the sentences in the dataset become the same length, which will make it easier for the model to learn how to summarize text."]},{"cell_type":"code","execution_count":null,"id":"5a91f338-377c-4b1a-a724-a44d5bf535c4","metadata":{"colab":{"referenced_widgets":[""]},"id":"5a91f338-377c-4b1a-a724-a44d5bf535c4","outputId":"58b856c6-51a5-4dbb-9eab-22fd3c3ea805"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/15551 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Maxium input length: 512 tokens\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/15551 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Maxium output length: 95 tokens\n"]}],"source":["from datasets import concatenate_datasets\n","\n","# Combined train and test datasets\n","combined_dataset = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]])\n","\n","# Tokenize the dialogue column and find the maximum length\n","tokenized_inputs = combined_dataset.map(lambda x: tokenizer(x[\"dialogue\"], truncation=True), batched=True, remove_columns=[\"dialogue\", \"summary\"])\n","max_input_length = max([len(x) for x in tokenized_inputs[\"input_ids\"]])\n","print(f\"Maxium input length: {max_input_length} tokens\")\n","\n","# Tokenize the summary column and find the maximum length\n","tokenized_targets = combined_dataset.map(lambda x: tokenizer(x[\"summary\"], truncation=True), batched=True, remove_columns=[\"dialogue\", \"summary\"])\n","max_output_length = max([len(x) for x in tokenized_targets[\"input_ids\"]])\n","print(f\"Maxium output length: {max_output_length} tokens\")"]},{"cell_type":"code","execution_count":null,"id":"d1a2b903-2e6f-4065-b770-113a9bd75656","metadata":{"colab":{"referenced_widgets":[""]},"id":"d1a2b903-2e6f-4065-b770-113a9bd75656","outputId":"b3971080-1687-4287-889e-7ad989c933b9"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/14732 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/819 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/818 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Keys of tokenized dataset: ['input_ids', 'attention_mask', 'labels']\n"]}],"source":["def preprocess_func(sample, padding=True):\n","    # add prefix to convert the dialogue to prompt\n","    prompts = [f\"summarize: {item}\" for item in sample[\"dialogue\"]]\n","\n","    # Tokenize the prompts as inputs\n","    model_inputs = tokenizer(prompts, max_length=max_input_length, padding=padding, truncation=True)\n","\n","    # Tokenize the summaries as labels, `text_target` keyword argument is used to tokenize targets\n","    labels = tokenizer(text_target=sample[\"summary\"], max_length=max_output_length, padding=padding, truncation=True)\n","\n","    # If padding is applied here, replace all tokenizer.pad_token_id in the labels with -100.\n","    # Those -100 will be ignored while computing evaluation metrics\n","    if padding:\n","        labels[\"input_ids\"] = [\n","            [(label if label != tokenizer.pad_token_id else -100) for label in label_list] for label_list in labels[\"input_ids\"]\n","        ]\n","\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs\n","\n","\n","# Create a tokenized dataset\n","tokenized_dataset = dataset.map(preprocess_func, batched=True, remove_columns=[\"dialogue\", \"summary\", \"id\"])\n","print(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")"]},{"cell_type":"markdown","id":"27851da5-226d-4a1d-8152-8fea9515f12c","metadata":{"id":"27851da5-226d-4a1d-8152-8fea9515f12c"},"source":["# Model"]},{"cell_type":"markdown","id":"83d6e41a-4731-4f5a-918b-db12ec4a7ff4","metadata":{"id":"83d6e41a-4731-4f5a-918b-db12ec4a7ff4"},"source":["## Load FLAN-T5 model\n","\n","Here you will load the model using the `MODEL_ID` defined earlier. As T5 is a Seq2Seq model, you will use **AutoModelForSeq2SeqLM** to load the correct model."]},{"cell_type":"code","execution_count":null,"id":"38cb51b3-e0d2-46dc-8961-46513884fa98","metadata":{"colab":{"referenced_widgets":["9e4f01ea0d9e4e55bf44b02e8cccc9a8","a18fd0a64bfb441d8cd9cc4bfac8801e","033c96b182ed4910a9e0ca25f5bf68fc"]},"id":"38cb51b3-e0d2-46dc-8961-46513884fa98","outputId":"57a30787-831d-4f1f-8729-dfbc544e4a9b"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9e4f01ea0d9e4e55bf44b02e8cccc9a8","version_major":2,"version_minor":0},"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/1.40k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a18fd0a64bfb441d8cd9cc4bfac8801e","version_major":2,"version_minor":0},"text/plain":["Downloading model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"033c96b182ed4910a9e0ca25f5bf68fc","version_major":2,"version_minor":0},"text/plain":["Downloading (…)neration_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from transformers import AutoModelForSeq2SeqLM\n","\n","# Load the model using pre-defined model ID\n","model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_ID)"]},{"cell_type":"markdown","id":"ee1cc0f5-38b6-46b9-8d10-1703912751a6","metadata":{"id":"ee1cc0f5-38b6-46b9-8d10-1703912751a6"},"source":["## Define evalution metrics\n","\n","Although a domain expert can evaluate the model's responses using their knowledge, it is better to have measurable metrics to compare the model responses. In this step, you will define the necessary functions to compare the metrics."]},{"cell_type":"code","execution_count":null,"id":"8a98ea1e-e753-4e5c-9c01-9d28d552514a","metadata":{"colab":{"referenced_widgets":["22b6b14554004cd29c18c6fd8809d8a7"]},"id":"8a98ea1e-e753-4e5c-9c01-9d28d552514a","outputId":"afe4dfa2-83ab-4ceb-db51-6a863bbdfc6e"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"22b6b14554004cd29c18c6fd8809d8a7","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["import evaluate\n","import nltk\n","import numpy as np\n","from nltk.tokenize import sent_tokenize\n","nltk.download(\"punkt\")\n","\n","# Use rouge metric\n","metric = evaluate.load(\"rouge\")\n","\n","# helper function to postprocess text\n","def postprocess_outputs(preds, labels):\n","    preds = [pred.strip() for pred in preds]\n","    labels = [label.strip() for label in labels]\n","\n","    # rougeLSum expects newline after each sentence\n","    preds = [\"\\n\".join(sent_tokenize(pred)) for pred in preds]\n","    labels = [\"\\n\".join(sent_tokenize(label)) for label in labels]\n","\n","    return preds, labels\n","\n","def compute_metrics(preds_and_labels):\n","    preds, labels = preds_and_labels\n","    if isinstance(preds, tuple):\n","        preds = preds[0]\n","\n","    # Batch decode the predictions using tokenizer\n","    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n","\n","    # Ignore -100 because they are paddings\n","    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","\n","    # Post-process the outputs, both labels and predictions\n","    decoded_preds, decoded_labels = postprocess_outputs(decoded_preds, decoded_labels)\n","\n","    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n","    result = {k: round(v * 100, 4) for k, v in result.items()}\n","    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n","    result[\"gen_len\"] = np.mean(prediction_lens)\n","    return result"]},{"cell_type":"markdown","id":"c46bcbd5-042a-4b44-b125-b37f97795bab","metadata":{"id":"c46bcbd5-042a-4b44-b125-b37f97795bab"},"source":["# Fine-tuning"]},{"cell_type":"markdown","id":"d100ff73-e4dc-429d-b18b-e9bea8d08e71","metadata":{"id":"d100ff73-e4dc-429d-b18b-e9bea8d08e71"},"source":["## Prepare for fine-tuning\n","\n","As mentioned earlier, if the samples in the training or validation dataset are shorter than the maximum input length, padding is required to ensure that the samples can be processed in batches. In this step, you will use the **DataCollatorForSeq2Seq** class to create a data collator that pads the samples."]},{"cell_type":"code","execution_count":null,"id":"eda63e83-2136-4f33-ab1f-d133a7fcdfd8","metadata":{"id":"eda63e83-2136-4f33-ab1f-d133a7fcdfd8"},"outputs":[],"source":["from transformers import DataCollatorForSeq2Seq\n","\n","# we want to ignore tokenizer pad token in the loss\n","label_pad_token_id = -100\n","\n","# Data collator\n","data_collator = DataCollatorForSeq2Seq(\n","    tokenizer,\n","    model=model,\n","    label_pad_token_id=label_pad_token_id,\n","    pad_to_multiple_of=8\n",")"]},{"cell_type":"markdown","id":"a320691b-d463-4670-9adb-74f4c6d411a5","metadata":{"id":"a320691b-d463-4670-9adb-74f4c6d411a5"},"source":["## Fine-tune the model\n","\n","Now, you have all the necessary components to begin the fine-tuning process. All that remains is to define the training arguments and initialize the trainer object. You will do both in the code cell below."]},{"cell_type":"code","execution_count":null,"id":"0bb36a21-7b27-48a3-a835-ac46d02ef1bf","metadata":{"id":"0bb36a21-7b27-48a3-a835-ac46d02ef1bf"},"outputs":[],"source":["import os\n","from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n","\n","LOGGING_DIR = os.path.join(MODEL_ID.split(\"/\")[-1], \"logs\")\n","OUTPUT_DIR = os.path.join(MODEL_ID.split(\"/\")[-1], \"output\")\n","\n","# Define training arguments\n","training_args = Seq2SeqTrainingArguments(\n","    # model traning parameters\n","    per_device_train_batch_size=24,\n","    per_device_eval_batch_size=24,\n","    predict_with_generate=True,\n","    fp16=False,\n","    learning_rate=5e-5,\n","    num_train_epochs=5,\n","\n","    # output & logging\n","    output_dir=OUTPUT_DIR,\n","    logging_dir=LOGGING_DIR,\n","    logging_strategy=\"steps\",\n","    logging_steps=500,\n","\n","    # evaluation strategies\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    save_total_limit=2,\n","    load_best_model_at_end=True,\n",")\n","\n","# Create trainer instance\n","trainer = Seq2SeqTrainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_dataset[\"validation\"],\n","    compute_metrics=compute_metrics,\n",")\n"]},{"cell_type":"markdown","id":"474e72a8-a7f6-40b9-862c-ea10271995d8","metadata":{"id":"474e72a8-a7f6-40b9-862c-ea10271995d8"},"source":["Here you will tune the model by training the model with the new dataset. It will take awhile."]},{"cell_type":"code","execution_count":null,"id":"5230ac01-f3ae-44bc-a4b1-347763e4a762","metadata":{"id":"5230ac01-f3ae-44bc-a4b1-347763e4a762","outputId":"e8a58802-ea2a-449a-de3c-bc1a6f4545b1"},"outputs":[{"name":"stderr","output_type":"stream","text":["You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1842' max='1842' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1842/1842 27:15, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Rouge1</th>\n","      <th>Rouge2</th>\n","      <th>Rougel</th>\n","      <th>Rougelsum</th>\n","      <th>Gen Len</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>1.464000</td>\n","      <td>1.398330</td>\n","      <td>47.994200</td>\n","      <td>24.796100</td>\n","      <td>40.584600</td>\n","      <td>44.670500</td>\n","      <td>17.506112</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>1.402600</td>\n","      <td>1.385565</td>\n","      <td>48.078100</td>\n","      <td>24.806500</td>\n","      <td>40.466900</td>\n","      <td>44.570100</td>\n","      <td>17.464548</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>1.361800</td>\n","      <td>1.387767</td>\n","      <td>48.199000</td>\n","      <td>25.224600</td>\n","      <td>40.841900</td>\n","      <td>44.859700</td>\n","      <td>17.405868</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["TrainOutput(global_step=1842, training_loss=1.3994161997245267, metrics={'train_runtime': 1638.5979, 'train_samples_per_second': 26.972, 'train_steps_per_second': 1.124, 'total_flos': 3.026353594879181e+16, 'train_loss': 1.3994161997245267, 'epoch': 3.0})"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["# Start the training\n","trainer.train()"]},{"cell_type":"markdown","id":"6fca3337-9fe6-4c53-9c07-82a28b8cc9c0","metadata":{"id":"6fca3337-9fe6-4c53-9c07-82a28b8cc9c0"},"source":["Here you will evaluate the fine-tuning model."]},{"cell_type":"code","execution_count":null,"id":"855ad233-72f5-4724-86e2-bad51dd7b9c8","metadata":{"id":"855ad233-72f5-4724-86e2-bad51dd7b9c8","outputId":"70e52280-4939-4a23-9409-9e9acfa8de94"},"outputs":[{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='35' max='35' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [35/35 00:27]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'eval_loss': 1.3855650424957275,\n"," 'eval_rouge1': 48.0781,\n"," 'eval_rouge2': 24.8065,\n"," 'eval_rougeL': 40.4669,\n"," 'eval_rougeLsum': 44.5701,\n"," 'eval_gen_len': 17.464547677261614,\n"," 'eval_runtime': 30.2485,\n"," 'eval_samples_per_second': 27.043,\n"," 'eval_steps_per_second': 1.157,\n"," 'epoch': 3.0}"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["trainer.evaluate()"]},{"cell_type":"markdown","id":"19032e22-9aef-43a5-89d9-c69d789f03f4","metadata":{"id":"19032e22-9aef-43a5-89d9-c69d789f03f4"},"source":["## Test the fine-tuned Model\n","\n","The moment of truth. Here you will test the tuned model with a random sample."]},{"cell_type":"code","execution_count":null,"id":"982c7cbc-a6a3-45bb-b9dd-cf37f459ac76","metadata":{"id":"982c7cbc-a6a3-45bb-b9dd-cf37f459ac76","outputId":"983c55e0-b935-4f12-d0d5-e7720e90e0e8"},"outputs":[{"name":"stdout","output_type":"stream","text":["DIALOGUE:\n","Wanda: Let's make a party!\n","Gina: Why?\n","Wanda: beacuse. I want some fun!\n","Gina: ok, what do u need?\n","Wanda: 1st I need too make a list\n","Gina: noted and then?\n","Wanda: well, could u take yours father car and go do groceries with me?\n","Gina: don't know if he'll agree\n","Wanda: I know, but u can ask :)\n","Gina: I'll try but theres no promisess\n","Wanda: I know, u r the best!\n","Gina: When u wanna go\n","Wanda: Friday?\n","Gina: ok, I'll ask\n","\n","GIVEN SUMMARY:\n","Wanda wants to throw a party. She asks Gina to borrow her father's car and go do groceries together. They set the date for Friday. \n","\n","flan-t5-base-tuned SUMMARY:\n","Wanda and Gina are going to make a party on Friday. Gina will take her father's car and go grocery shopping with her.\n"]}],"source":["from transformers import pipeline\n","from random import randrange\n","\n","# Create an inference pipeline with fine-tuned model and tokenizer\n","summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer, device=0)\n","\n","# select a test sample\n","sample = dataset['test'][10]\n","print(f\"DIALOGUE:\\n{sample['dialogue']}\\n\")\n","print(f\"GIVEN SUMMARY:\\n{sample['summary']}\\n\")\n","\n","# summarize dialogue using the base model\n","max_length = len(tokenizer(sample['dialogue'])['input_ids'])\n","response = summarizer(sample[\"dialogue\"], max_length=max_length)\n","\n","print(f\"flan-t5-base-tuned SUMMARY:\\n{response[0]['summary_text']}\")"]},{"cell_type":"markdown","id":"fe06fe65-dbe2-4c62-b94a-726e8486b968","metadata":{"id":"fe06fe65-dbe2-4c62-b94a-726e8486b968"},"source":["## Save the fine-tuned model & its tokenizer\n","\n","When you are satisfied with the model performance, run the cell below to save the model and its tokenizer to local disk. You can also upload the model and tokenizer to depoly on the cloud."]},{"cell_type":"code","execution_count":null,"id":"9ff6e79d-b3e0-49c3-9ab0-5195dcd59563","metadata":{"id":"9ff6e79d-b3e0-49c3-9ab0-5195dcd59563","outputId":"7768f8aa-b3b3-4907-e543-c38cff75b742"},"outputs":[{"data":{"text/plain":["('fine-tuned-flan-t5/tokenizer/tokenizer_config.json',\n"," 'fine-tuned-flan-t5/tokenizer/special_tokens_map.json',\n"," 'fine-tuned-flan-t5/tokenizer/tokenizer.json')"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["SAVE_DIR = \"fine-tuned-flan-t5\"\n","MODEL_DIR = os.path.join(SAVE_DIR, \"model\")\n","TOKENIZER_DIR = os.path.join(SAVE_DIR, \"tokenizer\")\n","\n","trainer.save_model(MODEL_DIR)\n","tokenizer.save_pretrained(TOKENIZER_DIR)"]},{"cell_type":"markdown","id":"61dc32a9-ad7d-4d66-bac4-50dfa9ab8744","metadata":{"id":"61dc32a9-ad7d-4d66-bac4-50dfa9ab8744"},"source":["## Load the saved model & its tokenizer\n","\n","In the next cell, you will reload the model and tokenzier from local disk and perform inference again."]},{"cell_type":"code","execution_count":null,"id":"2345f8bc-3b68-48f7-8507-aaf94113b69a","metadata":{"id":"2345f8bc-3b68-48f7-8507-aaf94113b69a"},"outputs":[],"source":["local_model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_DIR)\n","local_tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_DIR)"]},{"cell_type":"code","execution_count":null,"id":"b52afa47-623b-4e9b-ab29-fa90faadbe3f","metadata":{"id":"b52afa47-623b-4e9b-ab29-fa90faadbe3f","outputId":"800a2dc9-a851-454a-aafd-66080468044b"},"outputs":[{"name":"stdout","output_type":"stream","text":["DIALOGUE:\n","Wanda: Let's make a party!\n","Gina: Why?\n","Wanda: beacuse. I want some fun!\n","Gina: ok, what do u need?\n","Wanda: 1st I need too make a list\n","Gina: noted and then?\n","Wanda: well, could u take yours father car and go do groceries with me?\n","Gina: don't know if he'll agree\n","Wanda: I know, but u can ask :)\n","Gina: I'll try but theres no promisess\n","Wanda: I know, u r the best!\n","Gina: When u wanna go\n","Wanda: Friday?\n","Gina: ok, I'll ask\n","\n","GIVEN SUMMARY:\n","Wanda wants to throw a party. She asks Gina to borrow her father's car and go do groceries together. They set the date for Friday. \n","\n","flan-t5-base-tuned SUMMARY:\n","Wanda and Gina are going to make a party on Friday. Gina will take her father's car and go grocery shopping with her.\n"]}],"source":["# Create an inference pipeline with locally saved model and tokenizer\n","summarizer = pipeline(\"summarization\", model=local_model, tokenizer=local_tokenizer, device=0)\n","\n","# select a test sample\n","random_sample = dataset['test'][10]\n","print(f\"DIALOGUE:\\n{random_sample['dialogue']}\\n\")\n","print(f\"GIVEN SUMMARY:\\n{random_sample['summary']}\\n\")\n","\n","# summarize dialogue using the base model\n","max_length = len(tokenizer(random_sample['dialogue'])['input_ids'])\n","response = summarizer(random_sample[\"dialogue\"], max_length=max_length)\n","\n","print(f\"flan-t5-base-tuned SUMMARY:\\n{response[0]['summary_text']}\")"]},{"cell_type":"markdown","id":"e31f64a2-87e6-42d9-b06d-ff4bcc7ccf62","metadata":{"id":"e31f64a2-87e6-42d9-b06d-ff4bcc7ccf62"},"source":["# Conclusion\n","\n","This is end of the lab. Through this lab, you have worked through\n","- FLAN-T5 model summary, it's bias, risk, and limitations\n","- Checking for attached GPU to the instance\n","- Setting up the environment for the lab\n","- Download and preparing the dataset for the fine-tuning task\n","- Loading the model and defining evaluation metrics for the fine-tuning-task\n","- Testing, saving, and loading the models and its tokenizer\n","\n","Now, you are ready to fine-tune similar LLMs with your own dataset. Whenever you create a new AI model, please try to follow the [Responsible AI Practices](https://ai.google/responsibility/responsible-ai-practices/). Also, have a look at [Google AI Principles](https://ai.google/responsibility/principles/) if you are in need of guielines.\n","\n","If you want to learn more about T5 fine-tuning, this blog from Google Research is a good read."]},{"cell_type":"code","execution_count":null,"id":"8a0d9e4a-8622-4889-becd-af4ce3f5afc5","metadata":{"id":"8a0d9e4a-8622-4889-becd-af4ce3f5afc5"},"outputs":[],"source":[]}],"metadata":{"environment":{"kernel":"python3","name":"pytorch-gpu.1-13.m109","type":"gcloud","uri":"gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m109"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"},"colab":{"provenance":[{"file_id":"1wn1jp7hb_rPlPChK3nkgHtOQz0pfKIzA","timestamp":1691940066874},{"file_id":"1E6nBhnmt77x68UIriwfyRFzo6HGkrUtS","timestamp":1691474511238}]}},"nbformat":4,"nbformat_minor":5}